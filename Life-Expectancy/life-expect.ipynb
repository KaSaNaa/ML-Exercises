{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 5148.41185360395593306748\n",
      "Iteration    1 => Loss: 3235.55091165184740020777\n",
      "Iteration    2 => Loss: 2041.87377112154490532703\n",
      "Iteration    3 => Loss: 1296.87126159225840638101\n",
      "Iteration    4 => Loss: 831.80092696140832231322\n",
      "Iteration    5 => Loss: 541.39875409119906635169\n",
      "Iteration    6 => Loss: 359.99684973918056130060\n",
      "Iteration    7 => Loss: 246.62674383666094968248\n",
      "Iteration    8 => Loss: 175.72743974809259270842\n",
      "Iteration    9 => Loss: 131.34941448978884181997\n",
      "Iteration   10 => Loss: 103.53922121568797365398\n",
      "Iteration   11 => Loss: 86.08425313668719525140\n",
      "Iteration   12 => Loss: 75.10592201480093876853\n",
      "Iteration   13 => Loss: 68.18204987356072876992\n",
      "Iteration   14 => Loss: 63.79937154535650023490\n",
      "Iteration   15 => Loss: 61.01195678492182139507\n",
      "Iteration   16 => Loss: 59.22808622781101206556\n",
      "Iteration   17 => Loss: 58.07726049113832544890\n",
      "Iteration   18 => Loss: 57.32720098832210453565\n",
      "Iteration   19 => Loss: 56.83204047531999236753\n",
      "Iteration   20 => Loss: 56.49997446875310203040\n",
      "Iteration   21 => Loss: 56.27305668191329601768\n",
      "Iteration   22 => Loss: 56.11457375000028235945\n",
      "Iteration   23 => Loss: 56.00115127806705572766\n",
      "Iteration   24 => Loss: 55.91781418497627242914\n",
      "Iteration   25 => Loss: 55.85489241042252217539\n",
      "Iteration   26 => Loss: 55.80607985712944696388\n",
      "Iteration   27 => Loss: 55.76721449194762669777\n",
      "Iteration   28 => Loss: 55.73550979557550988375\n",
      "Iteration   29 => Loss: 55.70906901360941532175\n",
      "Iteration   30 => Loss: 55.68657686607593859662\n",
      "Iteration   31 => Loss: 55.66710283130019831788\n",
      "Iteration   32 => Loss: 55.64997476157132894059\n",
      "Iteration   33 => Loss: 55.63469698271503460774\n",
      "Iteration   34 => Loss: 55.62089665251528458612\n",
      "Iteration   35 => Loss: 55.60828817221512565538\n",
      "Iteration   36 => Loss: 55.59664921407802751219\n",
      "Iteration   37 => Loss: 55.58580429061009908764\n",
      "Iteration   38 => Loss: 55.57561327461591815791\n",
      "Iteration   39 => Loss: 55.56596321288066064881\n",
      "Iteration   40 => Loss: 55.55676236548747226607\n",
      "Iteration   41 => Loss: 55.54793577602653442682\n",
      "Iteration   42 => Loss: 55.53942191553606022580\n",
      "Iteration   43 => Loss: 55.53117009518307156668\n",
      "Iteration   44 => Loss: 55.52313844091718664231\n",
      "Iteration   45 => Loss: 55.51529228735854815113\n",
      "Iteration   46 => Loss: 55.50760289041595996196\n",
      "Iteration   47 => Loss: 55.50004638639105536413\n",
      "Iteration   48 => Loss: 55.49260294454574449219\n",
      "Iteration   49 => Loss: 55.48525607342922683074\n",
      "Iteration   50 => Loss: 55.47799205067510541767\n",
      "Iteration   51 => Loss: 55.47079945277384638302\n",
      "Iteration   52 => Loss: 55.46366876633220499571\n",
      "Iteration   53 => Loss: 55.45659206609404634492\n",
      "Iteration   54 => Loss: 55.44956274787720218455\n",
      "Iteration   55 => Loss: 55.44257530682167356417\n",
      "Iteration   56 => Loss: 55.43562515311209182300\n",
      "Iteration   57 => Loss: 55.42870845874797680608\n",
      "Iteration   58 => Loss: 55.42182203007273955109\n",
      "Iteration   59 => Loss: 55.41496320169432010516\n",
      "Iteration   60 => Loss: 55.40812974818491909446\n",
      "Iteration   61 => Loss: 55.40131981056589438595\n",
      "Iteration   62 => Loss: 55.39453183509329647904\n",
      "Iteration   63 => Loss: 55.38776452227985203081\n",
      "Iteration   64 => Loss: 55.38101678443901221272\n",
      "Iteration   65 => Loss: 55.37428771032343632896\n",
      "Iteration   66 => Loss: 55.36757653567165959885\n",
      "Iteration   67 => Loss: 55.36088261867450910358\n",
      "Iteration   68 => Loss: 55.35420541953932627166\n",
      "Iteration   69 => Loss: 55.34754448346706823259\n",
      "Iteration   70 => Loss: 55.34089942647205617732\n",
      "Iteration   71 => Loss: 55.33426992356961449104\n",
      "Iteration   72 => Loss: 55.32765569893621915298\n",
      "Iteration   73 => Loss: 55.32105651771239251957\n",
      "Iteration   74 => Loss: 55.31447217917443026636\n",
      "Iteration   75 => Loss: 55.30790251104630073087\n",
      "Iteration   76 => Loss: 55.30134736476074408529\n",
      "Iteration   77 => Loss: 55.29480661151236375872\n",
      "Iteration   78 => Loss: 55.28828013896883675216\n",
      "Iteration   79 => Loss: 55.28176784853183534096\n",
      "Iteration   80 => Loss: 55.27526965305494144332\n",
      "Iteration   81 => Loss: 55.26878547494263216322\n",
      "Iteration   82 => Loss: 55.26231524456655819222\n",
      "Iteration   83 => Loss: 55.25585889894637858788\n",
      "Iteration   84 => Loss: 55.24941638065072879726\n",
      "Iteration   85 => Loss: 55.24298763688157265506\n",
      "Iteration   86 => Loss: 55.23657261871149160015\n",
      "Iteration   87 => Loss: 55.23017128044839552103\n",
      "Iteration   88 => Loss: 55.22378357910604762537\n",
      "Iteration   89 => Loss: 55.21740947396318688334\n",
      "Iteration   90 => Loss: 55.21104892619601400838\n",
      "Iteration   91 => Loss: 55.20470189857231702035\n",
      "Iteration   92 => Loss: 55.19836835519625850566\n",
      "Iteration   93 => Loss: 55.19204826129609386953\n",
      "Iteration   94 => Loss: 55.18574158304719645685\n",
      "Iteration   95 => Loss: 55.17944828742442098246\n",
      "Iteration   96 => Loss: 55.17316834207930043021\n",
      "Iteration   97 => Loss: 55.16690171523787000751\n",
      "Iteration   98 => Loss: 55.16064837561498279683\n",
      "Iteration   99 => Loss: 55.15440829234358943722\n",
      "\n",
      "Weights: [[0.02375353 0.05278059 0.14657346 0.7650678 ]]\n",
      "\n",
      "A few predictions:\n",
      "X[0] -> 77.1124 (label: 76)\n",
      "X[1] -> 78.5113 (label: 74)\n",
      "X[2] -> 78.2247 (label: 82)\n",
      "X[3] -> 79.0417 (label: 81)\n",
      "X[4] -> 68.3356 (label: 71)\n",
      "X[5] -> 77.1656 (label: 75)\n",
      "X[6] -> 80.4451 (label: 76)\n",
      "X[7] -> 70.9795 (label: 71)\n",
      "X[8] -> 78.1985 (label: 75)\n",
      "X[9] -> 78.1921 (label: 72)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21372\\3077580947.py:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"X[%d] -> %.4f (label: %d)\" % (i, predict(X[i], w), Y[i]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# computing the predictions\n",
    "def predict(X, w):\n",
    "    return np.matmul(X, w)\n",
    "\n",
    "# calculating the loss\n",
    "def loss(X, Y, w):\n",
    "    return np.average((predict(X, w) - Y) ** 2)\n",
    "\n",
    "# evaluating the gradient\n",
    "def gradient(X, Y, w):\n",
    "    return 2 * np.matmul(X.T, (predict(X, w) - Y)) / X.shape[0]\n",
    "\n",
    "# performing the training phase for our classifier\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration %4d => Loss: %.20f\" % (i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w\n",
    "\n",
    "# loading the data first and then training the classifier for 5000000 iteration\n",
    "FILENAME = \"life-expectancy-without-country-names.txt\"\n",
    "x1, x2, x3, y = np.loadtxt(FILENAME, skiprows=1, unpack=True)\n",
    "X = np.column_stack((np.ones(x1.size), x1, x2, x3))\n",
    "Y = y.reshape(-1, 1)\n",
    "w = train(X, Y, iterations=100, lr=0.0001)\n",
    "\n",
    "print(\"\\nWeights: %s\" % w.T)\n",
    "print(\"\\nA few predictions:\")\n",
    "for i in range(10):\n",
    "    print(\"X[%d] -> %.4f (label: %d)\" % (i, predict(X[i], w), Y[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
